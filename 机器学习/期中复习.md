## 1. 课程概览

#### 1.1 机器学习与人工智能、深度学习三者的关系、区别

- 计算机系统对人类智能过程的模拟。人工智能是一个愿景/目标.

- 实现人工智能的方法统称为机器学习，简单来说就是从历史数据中学习规律，然后训练出模型，使用模型预测未来的一种方法. 机器从经验中自动学习和改进的过程，不需要人工编写程序指定规则和逻辑。

- **深度学习是用于建立、模拟人脑进行分析学习的神经网络，并模仿人脑的机制来解释数据的一种机器学习技术**.
  
  属于机器学习中的一类方法. 深度学习可以自动提取特征，将简单的特征组合成复杂的特征，也就是说，通过逐层特征转换，将样本在原空间的特征转换为更高维度空间的特征，从而使分类或预测更加容易。与人工提取复杂特征的方法相比，利用大数据来学习特征，能够更快速、方便地刻画数据丰富的内在信息。

#### 1.2 机器学习三步骤

- Model: 定义函数集合

- Goodness of function: 模型评估-使用损失函数评估函数优度

- Bset Function: 从中选出最优函数及参数-使用梯度下降

## 2. 回归

#### 2.1 解释过拟合和欠拟合现象

- 欠拟合是指模型不能在训练集上获得足够低的误差, 模型在训练集上就表现很差，没法学习到数据背后的规律

- 过拟合是指训练误差和测试误差之间的差距太大。就是模型复杂度高于实际问题，**模型在训练集上表现很好，但在测试集上却表现很差**。模型**泛化能力差**。

#### 2.2 解释方差和偏差的概念

- 训练数据集的损失与一般化的数据集的损失之间的差异就叫做**泛化误差（generalization error）**。而泛化误差可以分解为**偏差（Biase）**、**方差(Variance)** 和 **噪声(Noise)**.

- **偏差的含义：偏差度量了学习算法的期望预测与真实结果的偏离程度，即刻画了学习算法本身的拟合能力。**

- **方差的含义：方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响。**

- **噪声的含义：噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。**

#### 2.3 正则化的概念和目的

在损失函数上加一个正则化项,限制模型中高次项系数的增长,从而增加模型的泛化能力, 避免过拟合现象.

L1,L2

#### 2.4 简述几种降低过拟合和欠拟合风险的方法

- 欠拟合
  
  - 增加特征数量
  
  - 模型复杂化

- 过拟合
  
  - **训练数据集样本单一，样本不足**: 增加训练数据
  
  - 训练数据中噪声干扰
  
  - 模型过于复杂: 采用合适模型,降低特征数量
  
  - 通用: 正则化
  
  - 提前终止
  
  - Dropout

## 3. 梯度下降

#### 3.1 简述梯度下降的过程

1. 确定当前模型参数值

2. 计算当前参数下的梯度
   
   $$
   \nabla L(\theta)=\begin{pmatrix}
\frac{\partial L}{\theta_1}\\
\frac{\partial L}{\theta_2}\\
\cdots\\
\frac{\partial L}{\theta_n}\\
\end{pmatrix}
   $$

3. 计算学习率  $\eta$

4. 更新模型参数
   
   $\theta_{t+1}=\theta_{t}-\eta \cdot \nabla L(\theta)$

#### 3.2 Adagrad方法解决了什么问题，如何做的？

(1) 最一般梯度下降学习率为定值,当模型训练到一定程度后会在最优解附近来回震荡.(2) 且不同参数适用的学习率的大小不一样，有些参数可能已经接近最优, 而有些参数还需要大幅度调动。

而Adagrad方法能够自适应不同的学习率, 且学习率随训练次数的增长不断下降.

#### 3.3 随机梯度下降解决了什么问题，如何做的？

使用全部数据进行训练时间开销大,训练效率慢.

随机梯度下降每次随机从样本集中抽取一个样本更新参数,而非全部数据.

随机梯度下降在大大提高了训练效率的同时, 训练精度没有下降特别多,同时还能有效避免模型陷入局部最优解.

## 4. 分类

#### 4.1 简述机器学习任务中，回归和分类任务的区别

不管是分类，还是回归，其本质是一样的，**都是对输入做出预测**，并且都是**监督学习**。

目的不同**分类**的目的是为了**寻找决策边界**，即分类算法得到是一个决策面，用于对数据集中的数据进行分类。**回归**的目的是为了**找到最优拟合**，通过回归算法得到是一个最优拟合线，这个线条可以最好的接近数据集中的各个点.

#### 4.2 分类和回归任务的模型输出区别

**分类问题输出的是物体所属的类别，回归问题输出的是物体的值**. 

**分类问题输出的值是定性的，回归问题输出的值是定量的**.

**分类问题输出的值是离散的，回归问题输出的值是连续的**.

#### 4.3 简述判别模型和生成模型各自的做法以及两种方法的区别

- 生成模型先假设数据符合分布,根据分布特征计算联合概率, 最后根据贝叶斯公式计算后验概率.

- 判别模型不对数据做任何假设,决策函数直接从数据中学习知识, 作为预测的模型.

- 区别: 是否假设数据分布. 一般生成模型更准确.分布假设越准确则生成模型越准确.

#### 4.4 逻辑斯蒂回归中，若使用平法误差(square error)作为损失函数的表示，可行吗？

不可.

当预测值$f_{w,b}(x)$接近0或1时,$\sigma^{'}(z)$会很小接近0,使得梯度很小,损失函数收敛慢.

## 5. 深度学习

#### 5.1 简述深度学习训练时，反向传播都做了什么

深度学习使用反向传播进行梯度下降.**反向传播通过导数链式法则计算损失函数对各参数的梯度，并根据梯度进行参数的更新.**

## 6. CNN

#### 6.1 简述CNN的卷积层和池化层，各自的实现了什么特性，以及做了什么

卷积层: **通过卷积操作对输入图像进行降维和特征抽取**(1) 简化:一个神经元检测一个小特征(2)去重: 减少检测相同特征的神经元.

池化层: 不改变矩阵数,缩小每一个矩阵大小. 对数据进一步降维,减少参数数量, 同时也避免过拟合.

#### 6.2 CNN中，卷积操作的本质特性包括稀疏交互和参数共享，具体解释这两种特性及其作用

稀疏交互: 一个神经元只与上一层的部分神经元相连.

作用: 减少参数数量, 缓解过拟合.

参数共享: 同一层的神经元受到上一层神经元影响相同, 权重相同.

作用: 大大减小模型参数数量, 减小训练开销.

## 7. 神经网络训练技巧

](https://so.csdn.net/so/search?q=%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B&spm=1001.2101.3001.7020)#### 7.1 梯度消失产生的原因以及如何解决

梯度趋近于零，网络权重无法更新或更新的很微小，网络训练再久也不会有效果；

通过梯度反向传播的方式计算梯度，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要链式法则（Chain Rule）的帮助，因此反向传播算法可以说是梯度下降在链式法则中的应用。而链式法则是一个**连乘的形式**，所以当层数越深的时候，梯度将以指数形式传播。

解决: (1)更换新的激励函数

#### 7.2 冲量(momentum)方法做了什么以及解决了什么问题

$v_{t+1}=\lambda\cdot v_{t}-\eta\cdot \nabla L(\theta_{t}) $

$\theta_{t+1}=\theta_{t}+v_{t+1}$

(1) 当遇到一个局部最优的时候有可能在原有动量的基础上冲出这个局部最优点；

(2) 物体运动方向由动量和梯度共同决定，可以使得物体的震荡减弱，更快地运动到最优解.

#### 7.3 dropout方法做了什么以及解决了什么问题

每次参数更新前,输入输出神经元保持不变, 其余每个神经元以 p 的概率删除,得到新的神经网络.

然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数 (w,b).

解决过拟合

#### 7.4 L1&L2 正则化

解决过拟合


