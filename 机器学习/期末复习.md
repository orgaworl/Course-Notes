# 期末复习

---

**解释过拟合和欠拟合现象**
过拟合：在训练集上拟合的较好，但在测试集上拟合的较差；欠拟合：在训练集和测试集上拟合的都很差

**解释方差和偏差的概念**

方差是预测值的变化范围，离散程度，即这些预测值离他们的期望值的距离；偏差是预测值的期望与真实值的差距。

**正则化的概念和目的**
正则化是指在损失函数中加入一些限制，即正则项w，通过这种规则去规范在接下来的循环迭代中不要自我膨胀。目的是防止过拟合

**简述几种降低过拟合和欠拟合风险的方法**
防止过拟合：减少模型参数，降低模型复杂度；正则化
防止欠拟合：增加模型参数，提高模型复杂度；增加样本数量；增加循环次数；查看是否学习率过高.

**简述梯度下降的过程**

1.随机初始化神经网络的权重和偏置 

2.输入训练数据，并通过前向传播计算神经网络的输出结果 

3.计算神经网络输出结果与真实值之间的误差值，并根据误差值计算梯度
4.根据梯度的大小和方向，调整神经网络的权重和偏置

5.重复2-4，直到误差小于阈值或者达到最大迭代次数

**Adagrad方法解决了什么问题，如何做的？**
Adagrad：不同的参数梯度不同，就需要不同的学习率来进行训练。可以帮助在梯度大的地方，调小学习率；在梯度小的地方，调大学习率。η/根号s+a  a是为了稳定性 每次将参数的梯度平方和加到s上

**随机梯度下降解决了什么问题，如何做的？**
随机梯度下降：这样的方法更快，更快收敛；每次更新时用一个样本。随机也就是说我们用样本中的一个例子来近似我所有的样本，来调整θ

---

**简述机器学习任务中，回归和分类任务的区别**

回归会给出一个具体的结果，分类作出二分类或者多分类的选择。

**分类和回归任务的模型输出区别**
回归是定量输出，也就是连续变量预测；分类是定性输出，也就是离散变量预测.

**简述判别模型和生成模型各自的做法以及两种方法的区别**
判别模型：根据训练数据得到分类函数和分界面，然后直接计算条件概率，我们将最大的作为新样本的分类。
生成模型：一般会对每一个类建立一个模型，有多少个类别，就建立多少个模型。计算新样本跟三个类别的联合概率，然后根据贝叶斯公式：分别计算，选择三类中最大的作为样本的分类。区别：判别式模型是在寻找一个决策边界，通过该边界来将样本划分到对应类别。而生成式模型则不同，它学习了每个类别的边界，它包含了更多信息，可以用来生成样本。

**逻辑斯蒂回归中，若使用平法误差(square error)作为损失函数的表示，可行吗**

---

**简述深度学习训练时，反向传播都做了什么**

反向传播通过导数链式法则计算损失函数对各参数的梯度，并根据梯度进行参数的更新.

**简述CNN的卷积层和池化层，各自的实现了什么特性，以及做了什么**

**CNN中，卷积操作的本质特性包括稀疏交互和参数共享，具体解释这两种特性及其作用**

对于全连接网络，任意一对输入与输出神经元之间都产生交互，形成稠密的连接结构。

在卷积神经网络中，卷积核尺度远远小于输入的维度，这样每个输出神经元仅与前一层特定局部区域内的神经元存在连接权重（即产生交互），我们就称这种特性为稀疏交互。先学习局部的特征，再将局部的特征组合起来形成更复杂和抽象的特征。
参数共享是指在同一个模型的不同模块中使用相同的参数，它是卷积运算的 固有属性。全连接网络中，计算每层的输出时，权值参数矩阵中的每个元素只作用于某个输入元素一次；而在卷积神经网络中，卷积核中的每一个元素将作用于 每一次局部输入的特定位置上。使得卷积层具有平移等变性

**梯度消失产生的原因以及如何解决**

更新权重的梯度不仅与本层的输入X有关还和下一层传过来的梯度有关。
需要知道梯度在过去的一段时间内的大致走向，以消除当前轮迭代梯度向量存在的方向抖动。某些梯度分量的值比另外一些分量的值要大的多，导致个别分量主导了梯度的更新方向，而期望的梯度更新方向却行进的非常缓慢.

**冲量(momentum)方法做了什么以及解决了什么问题**

**dropout方法做了什么以及解决了什么问题**

**L1&L2 正则化**

---

## RNN

### 课程

**梯度**

- clipping技术解决梯度爆炸.

- LSTM 防止梯度消失.
  
  forget gate设置高bias, 保证在多数情况下开启

**tec**

- GRU： 比LSTM更简化, 参数少，训练快，避免过拟合。

- Clockwise RNN

- Structurally Constrained RN

**RNN 应用**

1. 多对一
   
   - 语义分析
   
   - 关键词提取

2. 多对多(输出较短)
   
   - 语音识别
     
     CTC(Connectionist Temporal Classification)解决叠词问题

3. 多对多(输出长度不确定)
   
   - 语法解析

### 问题

**1. 相比CNN与传统DNN，RNN是为了解决什么问题？如何解决的？**

****无法对时间序列上的变化进行建模.**** RNN可以在每一步的输入上考虑前面的信息，从而对每个时间步生成一个输出.

RNNs是一种神经网络架构，专门用于处理具有时间序列或序列数据的问题。其特点在于能够捕捉序列中的长期依赖关系。

这种特性使得 RNN 非常适用于处理序列数据和预测问题。 在应用层面：前馈神经网络适用于简单的回归和分类问题，而循环神经网络适用于处理序列数据和预测问题。

**2. 长短时记忆网络LSTM四个主要的模块及其各自作用**

记忆单元: 能够在时间序列中长时间保留信息.

输出门: 决定了下一个隐藏状态 (也即下一个时间步的输出) .

输入门: 决定了哪些新信息将被存储在记忆单元中.

遗忘门: 决定哪些信息从记忆单元中遗忘.

**3. RNN训练中为什么会出现梯度爆炸现象**

**梯度消失:** 参数无法更新，最终导致训练失败

**梯度爆炸:** 梯度过大，大幅更新模型参数，模型不稳定.

梯度消失还是梯度爆炸，都是**源于网络结构太深**，造成网络权重不稳定，从本质上来讲是**因为梯度反向传播中的连乘效应。**

梯度消失: ReLu激活函数, LSTM神经元

-

相比CNN和传统DNN，RNN（循环神经网络）是为了解决序列数据处理问题而设计的。传统的前馈神经网络（如DNN）无法处理具有时序关系的数据，因为它们将每个输入和输出视为独立的数据点。而RNN引入了时间上的循环连接，使得网络可以保留先前的信息并在后续步骤中进行传递。例如自然语言处理、语音识别、机器翻译等任务。
循环神经网络的隐藏层之间的结点是有连接的，隐藏层的输入不仅包括输入层的输出，还包括上时刻隐藏层的输出。
输入门：决定当前时刻网络的输入数据有多少需要保存到单元状态。
遗忘门：决定上一时刻的单元状态有多少需要保留到当前时刻。
输出门：控制当前单元状态有多少需要输出到当前的输出值。
记忆单元：负责存储和传递信息
求偏导后有tanh’w累乘，如果w很大，累乘后趋近于无穷。

---

## Semi-supervised

### 知识点

**分类**

（1）归纳学习Inductive learning：又叫纯半监督学习，未知标签的数据不作为测试集数据

（2） 直推学习Transductive learning ：将 未知标签的数据 直接作为 测试集数据（用了未知标签的数据的feature）

1. 半监督生成模型
   
   使用soft label，样本属于每个label的概率.

2. Low-density Sparation 低密度分离假设-非黑即白
   
   使用hard label, 0或1，表示是否属于这个组
   
   1. 自训练 Self-training
   
   2. Entropy-based Regularization
   
   3. 半监督的SVM

3. Smothness Assumption
   
   1. 基于密度的方式Cluster and then Label 聚类和标记
   
   2. Graph-bases Approach

### 问题

**1. 半监督学习中的生成模型方法和监督学习中的有何异同？**

监督学习一步到位, 结果由数据集唯一确定.

半监督需要进行多次参数更新直到模型结果收敛,最终结果与模型初始值有关.

**2. Hard label / Soft label 区别**

Hard label取值只能0或1,表示是否属于该类别.

Softlabel取值可以为0-1之间的小数，表示属于该类别的概率.

-

同：都根据有标签数据和其对应的分布，根据贝叶斯公式，得到后验概率
异：监督学习这样就已经得到模型。但半监督学习还要根据后验概率对未标签数据进行分类，接着更新模型参数，迭代来优化模型

硬标签是一种二进制标签，用于明确指定每个样本的类别或目标值。
软标签是一种概率分布形式的标签，用于表示每个样本属于各个类别的概率。

---

## Unsupervised

### 问题

**1. 简述聚类中的k-means方法**

1. 选择k个随机样本为k个初始聚类中心.

2. 为每个点找到其最接近(中心点)的一个聚类，加入该聚类.

3. 更新每个聚类的中心点为该聚类中所有点的均值.

4. 不断重复2和3直到模型收敛或达到迭代次数上限..

**2. 简述PCA方法以及其实现的最主要目标**

将n维特征映射到k维上，这k维是全新的正交特征(主成分)，是在原有n维特征的基础上重新构造出来的k维特征

1) 去平均值，即每一位特征减去各自的平均值。

2) 计算协方差矩阵。

3) 通过SVD计算协方差矩阵的特征值与特征向量。

4) 对特征值从大到小排序，选择其中最大的k个。然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。

5) 将数据转换到k个特征向量构建的新空间中。

**3. 异常检测主要做了什么**

1. 给定一组训练集, 使用其训练一个auto-encoder和auto-decoder.
   
   要求decoder输出和输入尽可能相似.

2. 检测输入是否与训练数据相似, 即检测decoder输出是否和输入相似
   
   相似则为正常. 不相似则异常.

-

从数据中选择k个对象作为初始聚类中心;2.计算每个聚类对象到聚类中心的距离来划分;3.再次计算每个聚类中心4.迭代直到最优

PCA的主要思想是将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征。
将样本进行均值归0（demean），即所有样本减去样本的均值。样本的分布没有改变，只是将坐标轴进行了移动。找到样本点映射后方差最大的单位向量ω
PCA是一种常用的降维方法，其主要目标是从高维数据中提取出最重要的特征或成分。通俗地说，PCA可以将原始数据映射到一个新的低维空间中，并尽可能地保留原始数据的信息。

异常检测：用一组训练集来训练异常检测器，检测输入是否与训练集相似，如果相似则是正常；如果不同，则为异常。

---

## AI-Security
